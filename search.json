[{"title":"前向传播函数","url":"/posts/2208d482/","content":"前向传播函数是什么？什么是前向传播？前向传播（Forward Propagation）就是神经网络处理输入数据、产生输出的过程。\n生活类比想象一个工厂流水线：\n原材料（输入图像）    ↓工位1：提取边缘特征    ↓工位2：提取纹理特征    ↓工位3：提取高级语义    ↓质检：压缩特征    ↓包装：分类结果    ↓成品（预测类别）\n\n这个从原材料到成品的整个过程，就是前向传播。\n在代码中的体现def forward(self, x):    &quot;&quot;&quot;    前向传播函数    参数:        x: 输入张量，形状为 (batch_size, 3, 64, 64)    返回:        输出张量，形状为 (batch_size, num_classes)    &quot;&quot;&quot;    # 第一层卷积特征提取    x = self.conv1(x)    # 第二层卷积特征提取    x = self.conv2(x)    # 第三层卷积特征提取    x = self.conv3(x)    # 全局平均池化，将特征图压缩为1x1    x = self.global_avg_pool(x)    # 展平张量，从 (batch_size, 64, 1, 1) 变为 (batch_size, 64)    x = torch.flatten(x, 1)    # 全连接层输出分类结果    return self.fc(x)\n\n数据流动过程# 输入：一张64x64的彩色图像input_image = torch.randn(1, 3, 64, 64)  # [1, 3, 64, 64]# 前向传播output = model(input_image)# 数据变化过程：# [1, 3, 64, 64]  ← 输入图像#   ↓ conv1 (卷积+池化)# [1, 16, 32, 32]  ← 16个特征图，尺寸减半#   ↓ conv2 (卷积+池化)# [1, 32, 16, 16]  ← 32个特征图，尺寸再减半#   ↓ conv3 (卷积+池化)# [1, 64, 8, 8]   ← 64个特征图，尺寸继续减半#   ↓ global_avg_pool# [1, 64, 1, 1]   ← 压缩为1x1#   ↓ flatten# [1, 64]          ← 展平成向量#   ↓ fc# [1, 15]          ← 15个类别的得分\n\n前向传播 vs 反向传播前向传播（Forward）:输入 → 网络 → 输出用途：预测、推理反向传播（Backward）:输出 → 计算误差 → 更新参数用途：训练、优化\n\n对比：\n# 前向传播：预测output = model(input_image)  # 调用forward函数predicted_class = torch.argmax(output, dim=1)# 反向传播：训练loss = criterion(output, target)  # 计算损失loss.backward()  # 反向传播，计算梯度optimizer.step()  # 更新参数\n\n","categories":["AI&大模型"],"tags":["深度学习"]},{"title":"C语言通信包结构体","url":"/posts/e77b76b7/","content":"C语言通信包结构体在C语言网络通信（UDP&#x2F;TCP）中，设计包结构体（Packet Structure）是核心环节，直接关系到数据的正确传输、解析效率以及跨平台兼容性。以下是设计网络通信包结构体时必须关注的关键细节：\n先来看看正确的例子\n#include &lt;stdint.h&gt;// 1. push: 将当前的对齐设置压入栈中保存（保护现场）//    1: 临时设置为 1 字节对齐#pragma pack(push, 1)typedef struct &#123;    uint8_t  header;      // 1 字节    uint32_t length;      // 4 字节    uint8_t  data[10];    // 10 字节    // 总大小严格为 15 字节，无填充&#125; Packet;// 2. pop: 从栈中弹出之前的设置并恢复（恢复现场）#pragma pack(pop)\n\n字节对齐（Byte Alignment）与填充（Padding）这是网络通信中最容易踩坑的地方。为了提高CPU访问内存的效率，编译器默认会对结构体成员进行字节对齐（如4字节或8字节对齐），这会在成员之间插入不可见的“填充字节”。这样一来，发送端和接收端如果编译器设置不同、平台不同（32位 vs 64位），或者仅仅因为成员顺序不同，导致结构体实际占用的内存大小（sizeof）和内部布局不一致，解析出的数据就会完全错误。为了解决这个问题，我们可以强制将结构体进行1字节对齐，这样就避免了对“填充字节”错误解析而导致的通信错误。\n\n方法：使用 #pragma pack(push,1) 和 #pragma pack(pop)。\n\n\n\n#pragma pack(push,1) 可以直接理解为开始1字节对齐\n#pragma pack(pop) 可以理解为恢复本次设置的1字节对齐，它们互相配对\n\n所以它们组合一起用会记住进入当前作用域前的对齐值（无论是默认值还是用户自定义的值）用完后会精准恢复到进入前的状态。\n当然 如果不写push和pop也是可以的 如果不写 \n\npack(1) 强制设为 1 字节对齐。\npack() 直接恢复为编译器的默认值（通常是 4 或 8）。\n\n风险：如果你的头文件被包含在一个已经修改了对齐方式（例如 pack(2)）的代码块中，pack() 会错误地将其恢复为默认值，破坏上层代码的逻辑。\n所以为了代码的健壮性和维护 建议使用push pop写法\n\n数据类型的大小与确定性\n避免使用基础类型：不要直接使用 int, long, short，因为它们在不同平台上的大小可能不同（例如 long 在Windows 64位下是4字节，在Linux 64位下是8字节）。\n使用固定宽度类型：始终使用 &lt;stdint.h&gt; 中定义的固定宽度类型，如 uint8_t, int16_t, uint32_t, int64_t 等，确保在任何平台上大小一致。\n\n","categories":["经典技术栈"],"tags":["C语言","网络通信"]}]